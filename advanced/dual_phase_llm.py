"""
Dual-Phase LLM System
Phase 1: Auditor - Generates vulnerability hypotheses
Phase 2: Critic - Validates and filters false positives
"""

from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
import json
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from advanced.llm_providers import LLMClient, LLMProvider


@dataclass
class VulnerabilityHypothesis:
    """A vulnerability hypothesis generated by the auditor"""
    id: str
    severity: str  # critical, high, medium, low
    category: str  # reentrancy, access_control, logic_flaw, etc.
    description: str
    affected_code: str
    attack_scenario: str
    confidence: float  # 0.0 to 1.0
    evidence: List[str] = field(default_factory=list)
    critic_verdict: Optional[str] = None  # "confirmed", "rejected", "uncertain"
    critic_reasoning: Optional[str] = None


@dataclass
class DualPhaseResult:
    """Result from dual-phase analysis"""
    total_hypotheses: int
    confirmed_vulnerabilities: List[VulnerabilityHypothesis]
    rejected_hypotheses: List[VulnerabilityHypothesis]
    uncertain_findings: List[VulnerabilityHypothesis]
    false_positive_rate: float
    audit_time: float
    critic_time: float


class AuditorAgent:
    """
    Phase 1: Auditor
    Generates vulnerability hypotheses with high recall (catches everything)
    May have false positives - that's OK, critic will filter
    """

    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client

    def audit_contract(self,
                      contract_code: str,
                      contract_name: str,
                      static_analysis_results: Optional[Dict[str, Any]] = None) -> List[VulnerabilityHypothesis]:
        """
        Generate vulnerability hypotheses
        Focus on HIGH RECALL - catch everything, even uncertain findings
        """

        system_prompt = """You are an expert smart contract security auditor.
Your job is to find EVERY POSSIBLE vulnerability, even if you're not 100% certain.

Generate vulnerability hypotheses for:
- Reentrancy (classic, cross-function, read-only, cross-contract)
- Access control issues
- Integer overflow/underflow
- Flash loan attacks
- Oracle manipulation
- Front-running/MEV
- Governance attacks
- Logic flaws
- Business logic errors
- Edge cases

For EACH hypothesis, provide:
1. Severity (critical/high/medium/low)
2. Category
3. Clear description
4. Affected code location
5. Attack scenario
6. Confidence (0.0-1.0)

BE THOROUGH. It's better to flag a false positive than miss a real vulnerability.
"""

        user_prompt = f"""Analyze this smart contract for vulnerabilities:

Contract: {contract_name}

```solidity
{contract_code}
```
"""

        if static_analysis_results:
            user_prompt += f"""

Static Analysis Results:
{json.dumps(static_analysis_results, indent=2)}
"""

        user_prompt += """

Return a JSON array of vulnerability hypotheses. Each must have:
- id: unique identifier
- severity: critical/high/medium/low
- category: vulnerability type
- description: what the vulnerability is
- affected_code: specific code location
- attack_scenario: how to exploit it
- confidence: 0.0 to 1.0
- evidence: list of supporting evidence

Example format:
[
  {
    "id": "VULN-001",
    "severity": "high",
    "category": "reentrancy",
    "description": "Potential reentrancy in withdraw function",
    "affected_code": "withdraw() line 45",
    "attack_scenario": "Attacker can recursively call withdraw before balance update",
    "confidence": 0.85,
    "evidence": ["External call before state change", "No reentrancy guard"]
  }
]

Return ONLY the JSON array, no other text.
"""

        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            response = self.llm.chat_completion(
                messages=messages,
                temperature=0.7,  # Slightly higher for creativity
                max_tokens=4000
            )

            # Parse JSON response
            hypotheses = self._parse_audit_response(response)
            return hypotheses

        except Exception as e:
            print(f"Auditor error: {str(e)}")
            return []

    def _parse_audit_response(self, response: str) -> List[VulnerabilityHypothesis]:
        """Parse LLM response into vulnerability hypotheses"""

        # Extract JSON from response
        response = response.strip()

        # Remove markdown code blocks if present
        if response.startswith("```"):
            lines = response.split("\n")
            response = "\n".join(lines[1:-1])

        try:
            data = json.loads(response)

            hypotheses = []
            for item in data:
                hypothesis = VulnerabilityHypothesis(
                    id=item.get("id", f"VULN-{len(hypotheses)+1}"),
                    severity=item.get("severity", "medium"),
                    category=item.get("category", "unknown"),
                    description=item.get("description", ""),
                    affected_code=item.get("affected_code", ""),
                    attack_scenario=item.get("attack_scenario", ""),
                    confidence=float(item.get("confidence", 0.5)),
                    evidence=item.get("evidence", [])
                )
                hypotheses.append(hypothesis)

            return hypotheses

        except json.JSONDecodeError as e:
            print(f"Failed to parse auditor response: {e}")
            print(f"Response: {response[:500]}")
            return []


class CriticAgent:
    """
    Phase 2: Critic
    Validates auditor's hypotheses with high precision (few false positives)
    Acts as adversarial validator
    """

    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client

    def validate_hypothesis(self,
                           hypothesis: VulnerabilityHypothesis,
                           contract_code: str,
                           contract_name: str) -> Tuple[str, str]:
        """
        Validate a single vulnerability hypothesis
        Returns: (verdict, reasoning)
        verdict: "confirmed", "rejected", "uncertain"
        """

        system_prompt = """You are a critical security reviewer.
Your job is to VALIDATE vulnerability findings and eliminate false positives.

For each vulnerability hypothesis, you must:
1. Analyze the code in detail
2. Look for evidence that confirms OR refutes the hypothesis
3. Consider edge cases and assumptions
4. Give a clear verdict: CONFIRMED, REJECTED, or UNCERTAIN

Be SKEPTICAL. Only confirm if you have strong evidence.
Be PRECISE. Explain your reasoning clearly.
"""

        user_prompt = f"""Validate this vulnerability hypothesis:

Contract: {contract_name}

Hypothesis:
- ID: {hypothesis.id}
- Severity: {hypothesis.severity}
- Category: {hypothesis.category}
- Description: {hypothesis.description}
- Affected Code: {hypothesis.affected_code}
- Attack Scenario: {hypothesis.attack_scenario}
- Auditor Confidence: {hypothesis.confidence}
- Evidence: {', '.join(hypothesis.evidence)}

Contract Code:
```solidity
{contract_code}
```

Your task:
1. Examine the affected code carefully
2. Try to construct a concrete attack (or prove it's impossible)
3. Look for mitigations that the auditor might have missed
4. Give your verdict

Return a JSON object:
{{
  "verdict": "confirmed" | "rejected" | "uncertain",
  "reasoning": "Detailed explanation of why",
  "additional_evidence": ["any new evidence you found"],
  "severity_adjustment": "keep the severity or suggest new severity",
  "exploit_feasibility": 0.0 to 1.0
}}

Return ONLY the JSON, no other text.
"""

        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            response = self.llm.chat_completion(
                messages=messages,
                temperature=0.3,  # Lower for precision
                max_tokens=2000
            )

            # Parse response
            result = self._parse_critic_response(response)
            return result.get("verdict", "uncertain"), result.get("reasoning", "")

        except Exception as e:
            print(f"Critic error: {str(e)}")
            return "uncertain", f"Error during validation: {str(e)}"

    def _parse_critic_response(self, response: str) -> Dict[str, Any]:
        """Parse critic response"""

        response = response.strip()

        # Remove markdown code blocks if present
        if response.startswith("```"):
            lines = response.split("\n")
            response = "\n".join(lines[1:-1])

        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {
                "verdict": "uncertain",
                "reasoning": "Failed to parse critic response"
            }


class DualPhaseLLM:
    """
    Orchestrates the dual-phase LLM analysis
    Phase 1 (Auditor): High recall - finds everything
    Phase 2 (Critic): High precision - filters false positives
    """

    def __init__(self,
                 llm_client: LLMClient,
                 use_different_models: bool = False,
                 auditor_model: Optional[str] = None,
                 critic_model: Optional[str] = None):
        """
        Args:
            llm_client: LLM client for both agents
            use_different_models: If True, use different models for auditor/critic
            auditor_model: Model for auditor (if different)
            critic_model: Model for critic (if different)
        """
        self.auditor = AuditorAgent(llm_client)

        # Use same or different LLM for critic
        if use_different_models and critic_model:
            # Create new client with different model
            critic_llm = LLMClient(
                provider=llm_client.provider,
                api_key=llm_client.api_key,
                model=critic_model
            )
            self.critic = CriticAgent(critic_llm)
        else:
            self.critic = CriticAgent(llm_client)

    def analyze_contract(self,
                        contract_code: str,
                        contract_name: str,
                        static_analysis_results: Optional[Dict[str, Any]] = None,
                        confidence_threshold: float = 0.5) -> DualPhaseResult:
        """
        Run dual-phase analysis

        Args:
            contract_code: Solidity source code
            contract_name: Name of contract
            static_analysis_results: Results from Slither, etc.
            confidence_threshold: Minimum confidence for critic validation

        Returns:
            DualPhaseResult with confirmed/rejected/uncertain findings
        """

        print("="*70)
        print("DUAL-PHASE LLM ANALYSIS")
        print("="*70)

        # PHASE 1: AUDITOR - Generate hypotheses
        print("\n[PHASE 1] AUDITOR - Generating vulnerability hypotheses...")
        import time
        audit_start = time.time()

        hypotheses = self.auditor.audit_contract(
            contract_code=contract_code,
            contract_name=contract_name,
            static_analysis_results=static_analysis_results
        )

        audit_time = time.time() - audit_start

        print(f"  âœ“ Generated {len(hypotheses)} hypotheses in {audit_time:.1f}s")

        # PHASE 2: CRITIC - Validate each hypothesis
        print("\n[PHASE 2] CRITIC - Validating hypotheses...")
        critic_start = time.time()

        confirmed = []
        rejected = []
        uncertain = []

        for i, hypothesis in enumerate(hypotheses, 1):
            print(f"  [{i}/{len(hypotheses)}] Validating {hypothesis.id}...")

            # Only validate if auditor confidence meets threshold
            if hypothesis.confidence < confidence_threshold:
                uncertain.append(hypothesis)
                hypothesis.critic_verdict = "uncertain"
                hypothesis.critic_reasoning = "Below confidence threshold, skipped validation"
                continue

            verdict, reasoning = self.critic.validate_hypothesis(
                hypothesis=hypothesis,
                contract_code=contract_code,
                contract_name=contract_name
            )

            hypothesis.critic_verdict = verdict
            hypothesis.critic_reasoning = reasoning

            if verdict == "confirmed":
                confirmed.append(hypothesis)
                print(f"    âœ“ CONFIRMED: {hypothesis.description[:60]}")
            elif verdict == "rejected":
                rejected.append(hypothesis)
                print(f"    âœ— REJECTED: {hypothesis.description[:60]}")
            else:
                uncertain.append(hypothesis)
                print(f"    ? UNCERTAIN: {hypothesis.description[:60]}")

        critic_time = time.time() - critic_start

        print(f"\n  âœ“ Validation complete in {critic_time:.1f}s")

        # Calculate false positive rate (rejected / total)
        false_positive_rate = len(rejected) / len(hypotheses) if hypotheses else 0.0

        # Create result
        result = DualPhaseResult(
            total_hypotheses=len(hypotheses),
            confirmed_vulnerabilities=confirmed,
            rejected_hypotheses=rejected,
            uncertain_findings=uncertain,
            false_positive_rate=false_positive_rate,
            audit_time=audit_time,
            critic_time=critic_time
        )

        self._print_summary(result)

        return result

    def _print_summary(self, result: DualPhaseResult):
        """Print analysis summary"""

        print("\n" + "="*70)
        print("DUAL-PHASE ANALYSIS SUMMARY")
        print("="*70)

        print(f"\nTotal hypotheses generated: {result.total_hypotheses}")
        print(f"  âœ“ Confirmed:  {len(result.confirmed_vulnerabilities)}")
        print(f"  âœ— Rejected:   {len(result.rejected_hypotheses)}")
        print(f"  ? Uncertain:  {len(result.uncertain_findings)}")

        print(f"\nFalse positive rate: {result.false_positive_rate*100:.1f}%")
        print(f"Phase 1 (Auditor) time: {result.audit_time:.1f}s")
        print(f"Phase 2 (Critic) time:  {result.critic_time:.1f}s")

        if result.confirmed_vulnerabilities:
            print(f"\nðŸš¨ CONFIRMED VULNERABILITIES:")
            for vuln in result.confirmed_vulnerabilities:
                print(f"\n  [{vuln.severity.upper()}] {vuln.id}: {vuln.description}")
                print(f"    Location: {vuln.affected_code}")
                print(f"    Attack: {vuln.attack_scenario[:80]}...")
                print(f"    Critic: {vuln.critic_reasoning[:100]}...")

        print("\n" + "="*70)


# Example usage
if __name__ == "__main__":
    from advanced.llm_providers import LLMProvider

    # Example contract with vulnerability
    contract_code = """
    pragma solidity ^0.8.0;

    contract VulnerableBank {
        mapping(address => uint256) public balances;

        function deposit() public payable {
            balances[msg.sender] += msg.value;
        }

        function withdraw(uint256 amount) public {
            require(balances[msg.sender] >= amount, "Insufficient balance");

            // VULNERABLE: External call before state update
            (bool success, ) = msg.sender.call{value: amount}("");
            require(success, "Transfer failed");

            balances[msg.sender] -= amount;  // State update AFTER external call
        }

        function getBalance() public view returns (uint256) {
            return balances[msg.sender];
        }
    }
    """

    # Setup LLM client
    print("Testing Dual-Phase LLM Analysis...")
    print("="*70)

    # Check for API key
    import os
    api_key = os.getenv("XAI_API_KEY")

    if not api_key:
        print("Error: XAI_API_KEY not set")
        print("Set it in .env or export XAI_API_KEY=your-key")
        sys.exit(1)

    llm = LLMClient(
        provider=LLMProvider.GROK,
        api_key=api_key
    )

    # Run dual-phase analysis
    analyzer = DualPhaseLLM(llm)

    result = analyzer.analyze_contract(
        contract_code=contract_code,
        contract_name="VulnerableBank",
        static_analysis_results=None,
        confidence_threshold=0.6
    )

    print("\nAnalysis complete!")
